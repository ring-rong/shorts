# AI YouTube Shorts Generator

[![Python Version](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) <!-- Или другая лицензия, если нужно -->

## Обзор

Этот проект автоматизирует создание коротких вертикальных видео (YouTube Shorts, TikTok, Reels) из более длинных видеороликов (локальных файлов или с YouTube). Он использует AI для анализа контента, выделения ключевых моментов и создания динамичного видеоряда с субтитрами и автоматическим кадрированием.

**Основной рабочий процесс:**

1.  **Ввод видео:** Пользователь указывает URL YouTube видео или выбирает локальный видеофайл.
2.  **Выбор аудио/субтитров:** Скрипт анализирует доступные аудиодорожки и (для MKV) дорожки субтитров, позволяя пользователю выбрать нужные.
3.  **Транскрипция/Извлечение субтитров:**
    *   Если выбрана дорожка субтитров (SRT/ASS), текст извлекается из нее.
    *   В противном случае аудиодорожка извлекается и транскрибируется с помощью `faster-whisper` для получения текста и временных меток слов.
4.  **AI Анализ (Deepseek):** Транскрипция (с временными метками) отправляется LLM (через Deepseek API) для:
    *   Генерации привлекательного заголовка.
    *   Генерации релевантных хэштегов.
    *   Идентификации наиболее интересных сегментов/сцен для включения в короткое видео.
5.  **Обработка сегментов:**
    *   Выделенные LLM сегменты вырезаются из исходного видео.
    *   Каждый сегмент обрезается до вертикального формата (9:16). При этом используется **отслеживание лиц (CSRT)** и **детекция речи (VAD)**: камера плавно панорамирует и масштабируется, чтобы удерживать группу лиц в кадре, отдавая приоритет предполагаемому говорящему (самому крупному лицу в кадре при наличии речи).
    *   На обрезанные сегменты накладываются **динамические субтитры**: если использовался Whisper, субтитры появляются группами по 1-3 слова; если использовались SRT-субтитры, отображается полная фраза сегмента.
6.  **Финальная сборка:** Обработанные вертикальные сегменты склеиваются с плавными переходами (fade in/out) в итоговое короткое видео.
7.  **Оптимизация:** Используется кэширование для результатов Whisper и LLM, а также частичное GPU-ускорение для детекции лиц (при наличии CUDA).

## Ключевые Возможности

*   **Источник видео:** YouTube URL или локальные видеофайлы (MP4, MKV, AVI, MOV и др.).
*   **Гибкий выбор аудио:** Автоматическое определение и выбор нужной аудиодорожки при наличии нескольких.
*   **Обработка субтитров:**
    *   Извлечение и использование текстовых субтитров (SRT/ASS/MOV_Text) из контейнеров MKV/MP4.
    *   Автоматическая транскрипция аудио с помощью `faster-whisper` при отсутствии подходящих субтитров.
*   **AI-анализ контента:** Использование LLM (Deepseek) для выделения ключевых моментов и генерации метаданных (заголовок, хэштеги).
*   **Динамическое кадрирование:** Автоматическое панорамирование и масштабирование для удержания лиц в вертикальном кадре (9:16) с приоритетом для говорящего.
*   **Динамические субтитры:** Генерация субтитров, синхронизированных по словам (из Whisper) или по фразам (из SRT).
*   **GPU Ускорение (частичное):** Использование CUDA (если настроено) для ускорения этапа детекции лиц (DNN).
*   **Кэширование:** Результаты транскрипции и LLM-анализа кэшируются для ускорения повторных запусков с тем же входным видео.

## Требования

### Программное обеспечение:

*   **Python:** 3.10 или 3.11 (из-за используемого wheel OpenCV).
*   **FFmpeg:** Должен быть установлен и доступен в системном `PATH`. Используется для извлечения аудио, субтитров, информации о потоках и других операций с видео/аудио. Скачать можно [здесь](https://ffmpeg.org/download.html).
*   **FFprobe:** Обычно поставляется вместе с FFmpeg. Используется для анализа медиафайлов.
*   **ImageMagick:** Требуется MoviePy для рендеринга текста (субтитров). Установите и убедитесь, что путь к исполняемому файлу `magick.exe` (или `convert` на Linux/macOS) указан в `.env`. Скачать можно [здесь](https://imagemagick.org/script/download.php).
*   **CUDA Toolkit:** Версия **12.1** (требуется для используемой сборки OpenCV). Скачать можно с [сайта NVIDIA](https://developer.nvidia.com/cuda-toolkit-archive).
*   **cuDNN:** Версия **8.9.2** для CUDA 12.x. Скачать можно с [сайта NVIDIA](https://developer.nvidia.com/cudnn) (требуется аккаунт разработчика).
    *   **Важно для Windows:** После скачивания архива cuDNN (`.zip` или `.tar`), его содержимое (папки `bin`, `include`, `lib`) нужно распаковать/скопировать в директорию установленного CUDA Toolkit (например, `C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.8/`). Либо следуйте инструкциям по настройке `PATH` из [репозитория opencv-python-cuda-wheels](https://github.com/cudawarped/opencv-python-cuda-wheels/releases/tag/4.11.0.20250124).

### Оборудование:

*   **NVIDIA GPU:** Требуется для работы CUDA-ускорения детекции лиц. Без совместимого GPU или без установленных CUDA/cuDNN, детекция будет выполняться на CPU.
*   **Достаточное количество RAM/VRAM:** Обработка видео и работа AI-моделей могут требовать значительных ресурсов.

## Установка

1.  **Клонируйте репозиторий:**
    ```bash
    git clone <URL вашего репозитория>
    cd <папка репозитория>
    ```
2.  **Установите CUDA и cuDNN:** Убедитесь, что у вас установлены CUDA Toolkit 12.1 и cuDNN 8.9.2, и они правильно настроены (особенно `PATH` и копирование файлов cuDNN для Windows).
3.  **Создайте и активируйте виртуальное окружение (рекомендуется):**
    ```bash
    python -m venv venv
    # Windows
    venv\Scripts\activate
    # Linux/macOS
    source venv/bin/activate
    ```
4.  **Установите зависимости:**
    ```bash
    pip install -r requirements.txt
    ```
    *   *Примечание:* Эта команда установит специальную сборку `opencv-contrib-python-cuda` с поддержкой CUDA 12.1 и соответствующую версию PyTorch. Стандартные `opencv-python` или `opencv-contrib-python` **не будут работать** с GPU-ускорением DNN. Убедитесь, что ваша версия Python соответствует версии в имени wheel-файла в `requirements.txt` (например, `cp311` для Python 3.11).
5.  **Проверьте FFmpeg/FFprobe:** Убедитесь, что команды `ffmpeg` и `ffprobe` доступны из вашей командной строки.
6.  **Скачайте модели:**
    *   Модель детекции лиц (`deploy.prototxt`, `res10_300x300_ssd_iter_140000_fp16.caffemodel`) должна находиться в папке `models/`. Если их там нет, их нужно скачать (например, из [репозитория OpenCV](https://github.com/opencv/opencv/tree/master/samples/dnn/face_detector)).
    *   Модели `faster-whisper` будут скачаны автоматически при первом запуске транскрипции.

## Конфигурация

Создайте файл `.env` в корневой папке проекта и добавьте в него следующие переменные:

```dotenv
# Пример для Windows:
IMAGEMAGICK_BINARY_PATH=C:\\Program Files\\ImageMagick-7.1.1-Q16-HDRI\\magick.exe
# Пример для Linux (может не требоваться, если в PATH):
# IMAGEMAGICK_BINARY_PATH=/usr/bin/convert

# Ваш API ключ от Deepseek (или другого OpenAI-совместимого API)
OPENAI_API=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

*   `IMAGEMAGICK_BINARY_PATH`: Полный путь к исполняемому файлу ImageMagick (`magick.exe` или `convert`).
*   `OPENAI_API`: Ваш API ключ для доступа к LLM Deepseek.

## Использование

Запустите главный скрипт из командной строки:

```bash
python main.py
```

Скрипт последовательно запросит:

1.  **Выбор локального видео:** Если в папке `local_videos` есть поддерживаемые видеофайлы, вам будет предложено выбрать один из них по номеру. Нажмите Enter, чтобы пропустить этот шаг и ввести URL YouTube.
2.  **URL YouTube:** Если локальное видео не выбрано, введите URL видео с YouTube.
3.  **Выбор аудиодорожки:** Если в видео несколько аудиодорожек, вам будет предложено выбрать нужную по индексу.
4.  **Выбор дорожки субтитров (если есть):** Если в видео найдены текстовые субтитры (SRT/ASS), вам будет предложено выбрать одну из них по индексу или ввести `s`/`skip` для использования транскрипции Whisper.

Далее скрипт автоматически выполнит все шаги обработки и сохранит результат в папку `final_output/`. Промежуточные файлы (обрезанные сегменты, аудио, отладочное видео с детекцией) создаются временно и удаляются после завершения (кроме папки `cache`).

## Структура Проекта
├── Components/ # Модули с основной логикой
│ ├── Edit.py # Функции обрезки видео/аудио (ffmpeg, moviepy)
│ ├── FaceCrop.py # Вертикальное кадрирование с трекингом и VAD
│ ├── LanguageTasks.py # Взаимодействие с LLM (Deepseek)
│ ├── Speaker.py # Детекция/трекинг лиц и VAD (OpenCV DNN, CSRT, webrtcvad)
│ ├── Subtitles.py # Извлечение/парсинг SRT субтитров (ffmpeg)
│ └── YoutubeDownloader.py # Загрузка видео с YouTube (pytubefix)
├── models/ # Модели (DNN для детекции лиц)
│ ├── deploy.prototxt
│ └── res10_300x300_ssd_iter_140000_fp16.caffemodel
├── cache/ # Кэш для Whisper и LLM
├── videos/ # Папка для скачанных YouTube видео
├── local_videos/ # Папка для ваших локальных видеофайлов
├── final_output/ # Папка для итоговых коротких видео
├── main.py # Главный исполняемый скрипт
├── requirements.txt # Зависимости Python
├── README.md # Этот файл
└── .env # Файл для переменных окружения (API ключи, пути) - создается вручную!


## Технические Детали

*   **Обработка видео:** Используются библиотеки `moviepy` и `opencv-python`.
*   **Транскрипция:** `faster-whisper`.
*   **Детекция лиц:** OpenCV DNN модуль с моделью Caffe (`res10_300x300_ssd`).
*   **Трекинг лиц:** `cv2.TrackerCSRT_create()` из `opencv-contrib-python`.
*   **Детекция речи:** `webrtcvad`.
*   **Анализ текста:** Deepseek API (OpenAI-совместимый).
*   **Загрузка YouTube:** `pytubefix`.

## GPU Ускорение

*   **Детекция лиц (DNN):** При наличии совместимой NVIDIA GPU, установленных CUDA/cuDNN и правильной сборки OpenCV (как в `requirements.txt`), этап *периодической детекции* лиц будет выполняться на GPU. Соответствующие сообщения должны появиться в логе при запуске.
*   **Трекинг лиц (CSRT):** Выполняется на CPU.
*   **Транскрипция (faster-whisper):** Использует GPU (CUDA), если PyTorch установлен с поддержкой GPU (как в `requirements.txt`).
*   **Кодирование видео (MoviePy):** По умолчанию выполняется на CPU (`libx264`). В коде используется попытка задействовать `h264_nvenc` (NVIDIA GPU кодер) для финальной записи, что может ускорить процесс, если FFmpeg собран с его поддержкой и драйверы установлены.

## Кэширование

*   Результаты транскрипции `faster-whisper` кэшируются в `cache/whisper_cache.json`. Ключ кэша зависит от хэша аудиофайла, используемой модели и промпта.
*   Результаты вызовов LLM Deepseek кэшируются в `cache/deepseek_cache.json`. Ключ зависит от хэша входного текста.
*   Это позволяет значительно ускорить повторные запуски скрипта на том же видео. Для сброса кэша просто удалите файлы из папки `cache/`.

## Известные Проблемы / Ограничения

*   **Качество видео:** Несмотря на установку битрейта, многократное перекодирование все еще может приводить к некоторой потере качества.
*   **Точность VAD/Приоритета говорящего:** Детекция речи (VAD) может быть неточной при наличии шума/музыки. Определение говорящего по размеру лица - это эвристика, которая может ошибаться.
*   **Точность трекинга:** CSRT трекер может терять объекты при быстрых движениях, сильных перекрытиях или изменениях освещения. Периодическая ре-детекция помогает, но не всегда спасает.
*   **Скорость:** Обработка, особенно трекинг и рендеринг видео, может быть длительной.
*   **Совместимость OpenCV:** Использование неофициальной сборки OpenCV с CUDA может потребовать точного соответствия версий CUDA, cuDNN и драйверов NVIDIA.

## Будущие Улучшения

*   **Точная диаризация:** Использовать `pyannote.audio` или другие библиотеки для точного определения, *кто* говорит в какой момент, и сопоставления с отслеживаемыми лицами.
*   **Улучшенный трекинг:** Интегрировать более продвинутые трекеры (DeepSORT + YOLO).
*   **Детекция смены сцен:** Автоматически определять резкие смены кадров и сбрасывать состояние камеры для более естественных переходов.
*   **Прямые вызовы FFmpeg:** Переписать узкие места (особенно `crop_to_vertical`) на прямые вызовы `ffmpeg` через `subprocess` для большего контроля над качеством и скоростью кодирования.
*   **Пользовательский интерфейс:** Создать GUI (например, с помощью Gradio, Streamlit или PyQt/Tkinter) для более удобного взаимодействия.
*   **Больше настроек:** Добавить параметры для управления качеством видео, агрессивностью VAD, параметрами трекера и т.д. через аргументы командной строки или UI.

## Лицензия

[MIT](https://opensource.org/licenses/MIT) <!-- Укажите вашу лицензию -->

---
